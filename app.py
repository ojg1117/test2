import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
import tempfile
import os

# =====================
# í˜ì´ì§€ ì„¤ì •
# =====================
st.set_page_config(
    page_title="PDF RAG ì±—ë´‡",
    page_icon="ğŸ“š",
    layout="wide"
)

# =====================
# ìŠ¤íƒ€ì¼ë§
# =====================
st.markdown("""
<style>
    .stChatMessage {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
    }
    .main-header {
        text-align: center;
        padding: 1rem 0;
        border-bottom: 2px solid #4A90A4;
        margin-bottom: 2rem;
    }
    .status-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #f0f2f6;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# =====================
# í—¤ë”
# =====================
st.markdown('<div class="main-header">', unsafe_allow_html=True)
st.title("ğŸ“š PDF RAG ì±—ë´‡")
st.caption("Powered by Gemini 2.5 Flash + LangChain")
st.markdown('</div>', unsafe_allow_html=True)

# =====================
# API Key ì„¤ì •
# =====================
try:
    GOOGLE_API_KEY = st.secrets["GEMINI_API_KEY"]
except KeyError:
    st.error("âš ï¸ `GEMINI_API_KEY`ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    st.stop()

# =====================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# =====================
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "messages" not in st.session_state:
    st.session_state.messages = []
if "chain" not in st.session_state:
    st.session_state.chain = None

# =====================
# PDF ì²˜ë¦¬ í•¨ìˆ˜
# =====================
@st.cache_resource
def get_embeddings():
    return GoogleGenerativeAIEmbeddings(
        model="models/text-embedding-004",
        google_api_key=GOOGLE_API_KEY
    )

def process_pdf(pdf_path: str):
    """PDFë¥¼ ë¡œë“œí•˜ê³  ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
    
    # PDF ë¡œë“œ
    loader = PyPDFLoader(pdf_path)
    documents = loader.load()
    
    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    splits = text_splitter.split_documents(documents)
    
    # ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    embeddings = get_embeddings()
    vectorstore = FAISS.from_documents(splits, embeddings)
    
    return vectorstore, len(splits)

def create_chain(vectorstore):
    """RAG ì²´ì¸ ìƒì„±"""
    
    # LLM ì´ˆê¸°í™” (Gemini 2.5 Flash)
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        google_api_key=GOOGLE_API_KEY,
        temperature=0.3,
        convert_system_message_to_human=True
    )
    
    # ë©”ëª¨ë¦¬ ì„¤ì •
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )
    
    # Conversational Retrieval Chain ìƒì„±
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
        memory=memory,
        return_source_documents=True,
        verbose=False
    )
    
    return chain

# =====================
# ì‚¬ì´ë“œë°” - PDF ì—…ë¡œë“œ
# =====================
with st.sidebar:
    st.header("ğŸ“„ PDF ì—…ë¡œë“œ")
    
    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=["pdf"],
        help="ì—…ë¡œë“œí•œ PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤."
    )
    
    # ê¸°ë³¸ test.pdf ì‚¬ìš© ì˜µì…˜
    use_default = st.checkbox(
        "ê¸°ë³¸ test.pdf ì‚¬ìš©",
        help="ì €ì¥ì†Œì— ìˆëŠ” test.pdf íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
    )
    
    process_btn = st.button("ğŸš€ PDF ì²˜ë¦¬ ì‹œì‘", type="primary", use_container_width=True)
    
    st.divider()
    
    # ìƒíƒœ í‘œì‹œ
    if st.session_state.vectorstore is not None:
        st.success("âœ… PDF ì²˜ë¦¬ ì™„ë£Œ!")
        st.info("ğŸ’¬ ì±„íŒ…ì°½ì—ì„œ ì§ˆë¬¸í•˜ì„¸ìš”.")
    
    # ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.messages = []
        st.session_state.chat_history = []
        st.session_state.chain = None
        st.rerun()

# =====================
# PDF ì²˜ë¦¬ ë¡œì§
# =====================
if process_btn:
    pdf_path = None
    
    # ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬
    if uploaded_file is not None:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            pdf_path = tmp_file.name
    # ê¸°ë³¸ íŒŒì¼ ì‚¬ìš©
    elif use_default and os.path.exists("test.pdf"):
        pdf_path = "test.pdf"
    else:
        st.sidebar.error("âš ï¸ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ê¸°ë³¸ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.")
    
    if pdf_path:
        with st.spinner("ğŸ“– PDFë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            try:
                vectorstore, num_chunks = process_pdf(pdf_path)
                st.session_state.vectorstore = vectorstore
                st.session_state.chain = create_chain(vectorstore)
                st.session_state.messages = []
                st.session_state.chat_history = []
                
                st.sidebar.success(f"âœ… ì²˜ë¦¬ ì™„ë£Œ! ({num_chunks}ê°œ ì²­í¬ ìƒì„±)")
                st.rerun()
                
            except Exception as e:
                st.sidebar.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                if uploaded_file is not None and pdf_path:
                    os.unlink(pdf_path)

# =====================
# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
# =====================
if st.session_state.vectorstore is None:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
else:
    # ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if "sources" in message:
                with st.expander("ğŸ“ ì°¸ì¡° ë¬¸ì„œ"):
                    st.markdown(message["sources"])

    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤” ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                try:
                    # RAG ì²´ì¸ ì‹¤í–‰
                    response = st.session_state.chain({
                        "question": prompt
                    })
                    
                    answer = response["answer"]
                    source_docs = response.get("source_documents", [])
                    
                    # ì‘ë‹µ í‘œì‹œ
                    st.markdown(answer)
                    
                    # ì°¸ì¡° ë¬¸ì„œ í‘œì‹œ
                    sources_text = ""
                    if source_docs:
                        with st.expander("ğŸ“ ì°¸ì¡° ë¬¸ì„œ"):
                            for i, doc in enumerate(source_docs, 1):
                                page_num = doc.metadata.get("page", "N/A")
                                content_preview = doc.page_content[:200] + "..."
                                source_info = f"**[{i}] í˜ì´ì§€ {page_num}**\n\n{content_preview}\n\n---\n"
                                st.markdown(source_info)
                                sources_text += source_info
                    
                    # ë©”ì‹œì§€ ì €ì¥
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": answer,
                        "sources": sources_text
                    })
                    
                except Exception as e:
                    error_msg = f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                    st.error(error_msg)
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": error_msg
                    })

# =====================
# í‘¸í„°
# =====================
st.divider()
st.caption("Made with â¤ï¸ using Streamlit, LangChain & Gemini 2.5 Flash")
