import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
import tempfile
import os

# =====================
# í˜ì´ì§€ ì„¤ì •
# =====================
st.set_page_config(
    page_title="ì˜¤ì­í˜•ì´ ë§Œë“  PDF RAG ì±—ë´‡",
    page_icon="ğŸ“š",
    layout="wide"
)

# =====================
# ìŠ¤íƒ€ì¼ë§
# =====================
st.markdown("""
<style>
    .stChatMessage {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
    }
    .main-header {
        text-align: center;
        padding: 1rem 0;
        border-bottom: 2px solid #4A90A4;
        margin-bottom: 2rem;
    }
</style>
""", unsafe_allow_html=True)

# =====================
# í—¤ë”
# =====================
st.markdown('<div class="main-header">', unsafe_allow_html=True)
st.title("ğŸ“š ì˜¤ì­í˜•ì´ ë§Œë“  PDF RAG ì±—ë´‡")
st.caption("Powered by Gemini 2.5 Flash + LangChain LCEL")
st.markdown('</div>', unsafe_allow_html=True)

# =====================
# API Key ì„¤ì •
# =====================
try:
    GOOGLE_API_KEY = st.secrets["GEMINI_API_KEY"]
except KeyError:
    st.error("âš ï¸ `GEMINI_API_KEY`ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    st.stop()

# =====================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# =====================
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "messages" not in st.session_state:
    st.session_state.messages = []
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# =====================
# LLM ë° ì„ë² ë”© ì´ˆê¸°í™”
# =====================
@st.cache_resource
def get_llm():
    return ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        google_api_key=GOOGLE_API_KEY,
        temperature=0.3
    )

@st.cache_resource
def get_embeddings():
    return GoogleGenerativeAIEmbeddings(
        model="models/text-embedding-004",
        google_api_key=GOOGLE_API_KEY
    )

# =====================
# PDF ì²˜ë¦¬ í•¨ìˆ˜
# =====================
def process_pdf(pdf_path: str):
    """PDFë¥¼ ë¡œë“œí•˜ê³  ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
    loader = PyPDFLoader(pdf_path)
    documents = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    splits = text_splitter.split_documents(documents)
    
    embeddings = get_embeddings()
    vectorstore = FAISS.from_documents(splits, embeddings)
    
    return vectorstore, len(splits)

# =====================
# RAG ì²´ì¸ í•¨ìˆ˜ (LCEL)
# =====================
def get_rag_response(query: str, vectorstore, chat_history: list):
    """LCEL ê¸°ë°˜ RAG ì‘ë‹µ ìƒì„±"""
    
    llm = get_llm()
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    
    # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    retrieved_docs = retriever.invoke(query)
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ PDF ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

[ì»¨í…ìŠ¤íŠ¸]
{context}"""),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}")
    ])
    
    # LCEL ì²´ì¸ êµ¬ì„±
    chain = prompt | llm | StrOutputParser()
    
    # ì²´ì¸ ì‹¤í–‰
    response = chain.invoke({
        "context": context,
        "chat_history": chat_history,
        "question": query
    })
    
    return response, retrieved_docs

# =====================
# ì‚¬ì´ë“œë°” - PDF ì—…ë¡œë“œ
# =====================
with st.sidebar:
    st.header("ğŸ“„ PDF ì—…ë¡œë“œ")
    
    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=["pdf"],
        help="ì—…ë¡œë“œí•œ PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤."
    )
    
    use_default = st.checkbox(
        "ê¹ƒí—ˆë¸Œì— ì—…ë¡œë„ëœ test.pdf ì‚¬ìš©",
        help="ê¹ƒí—ˆë¸Œ ì €ì¥ì†Œì— ìˆëŠ” test.pdf íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
    )
    
    process_btn = st.button("ğŸš€ PDF ì²˜ë¦¬ ì‹œì‘", type="primary", use_container_width=True)
    
    st.divider()
    
    if st.session_state.vectorstore is not None:
        st.success("âœ… PDF ì²˜ë¦¬ ì™„ë£Œ!")
        st.info("ğŸ’¬ ì±„íŒ…ì°½ì—ì„œ ì§ˆë¬¸í•˜ì„¸ìš”.")
    
    if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.messages = []
        st.session_state.chat_history = []
        st.rerun()

# =====================
# PDF ì²˜ë¦¬ ë¡œì§
# =====================
if process_btn:
    pdf_path = None
    
    if uploaded_file is not None:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            pdf_path = tmp_file.name
    elif use_default and os.path.exists("test.pdf"):
        pdf_path = "test.pdf"
    else:
        st.sidebar.error("âš ï¸ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ê¸°ë³¸ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.")
    
    if pdf_path:
        with st.spinner("ğŸ“– PDFë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            try:
                vectorstore, num_chunks = process_pdf(pdf_path)
                st.session_state.vectorstore = vectorstore
                st.session_state.messages = []
                st.session_state.chat_history = []
                
                st.sidebar.success(f"âœ… ì²˜ë¦¬ ì™„ë£Œ! ({num_chunks}ê°œ ì²­í¬ ìƒì„±)")
                st.rerun()
                
            except Exception as e:
                st.sidebar.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            finally:
                if uploaded_file is not None and pdf_path and os.path.exists(pdf_path):
                    os.unlink(pdf_path)

# =====================
# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
# =====================
if st.session_state.vectorstore is None:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
else:
    # ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if "sources" in message and message["sources"]:
                with st.expander("ğŸ“ ì°¸ì¡° ë¬¸ì„œ"):
                    st.markdown(message["sources"])

    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤” ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                try:
                    answer, source_docs = get_rag_response(
                        prompt,
                        st.session_state.vectorstore,
                        st.session_state.chat_history
                    )
                    
                    st.markdown(answer)
                    
                    # ì°¸ì¡° ë¬¸ì„œ í‘œì‹œ
                    sources_text = ""
                    if source_docs:
                        with st.expander("ğŸ“ ì°¸ì¡° ë¬¸ì„œ"):
                            for i, doc in enumerate(source_docs, 1):
                                page_num = doc.metadata.get("page", "N/A")
                                content_preview = doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content
                                source_info = f"**[{i}] í˜ì´ì§€ {page_num}**\n\n{content_preview}\n\n---\n"
                                st.markdown(source_info)
                                sources_text += source_info
                    
                    # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
                    st.session_state.chat_history.append(HumanMessage(content=prompt))
                    st.session_state.chat_history.append(AIMessage(content=answer))
                    
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": answer,
                        "sources": sources_text
                    })
                    
                except Exception as e:
                    error_msg = f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                    st.error(error_msg)
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": error_msg,
                        "sources": ""
                    })

# =====================
# í‘¸í„°
# =====================
st.divider()
st.caption("Made with â¤ï¸ using Streamlit, LangChain LCEL & Gemini 2.5 Flash")
